{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Scraper\n",
    "This function scrapes all the images on a webpage and stores them in a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageScraper(url,folder):\n",
    "    # Import the required libraries\n",
    "    import os # for creating directories and deleting files\n",
    "    import requests # for making HTTP requests\n",
    "    from bs4 import BeautifulSoup # for parsing html\n",
    "\n",
    "    # Define the URL for the site to be scraped \n",
    "    url = url\n",
    "    #print(url)\n",
    "    # Define the folder for the images to be stored in\n",
    "    folder = folder\n",
    "\n",
    "    # Do http GET request to harvest the sites html, and parse it using BeautifulSoup\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Extract a list of all the image URLs from the html 'soup'. These can be indexed using the 'src' tag.\n",
    "    imageurls = [img['src'] for img in soup.find_all('img')]\n",
    "\n",
    "    # Clean these URLs by adding 'https:' where necessary\n",
    "    cleanurls = []\n",
    "    for i in imageurls:\n",
    "        if i[:2] == '//':\n",
    "            cleanurls.append('https:'+ i)\n",
    "        elif i[:1] == '/':\n",
    "            cleanurls.append('https:/'+ i)\n",
    "        elif i[:4] != 'http':\n",
    "            cleanurls.append('https:/'+ i)\n",
    "        else:\n",
    "            cleanurls.append(i)\n",
    " \n",
    "    # Define a file path for images to be written to using the folder provided\n",
    "    imgsf = folder+'\\Imgs'\n",
    "    # Create the 'Imgs' directory if it does not exist already\n",
    "    if not os.path.exists(imgsf):\n",
    "        os.makedirs(imgsf)\n",
    "    \n",
    "    # Loop through the list of cleaned URLs, use a GET request to retrieve each image, then write them to directory  \n",
    "    n=0\n",
    "    for link in cleanurls:  \n",
    "        path = imgsf+'\\pic_{}.jpg'.format(n)\n",
    "        handle = open(path,'wb')\n",
    "    # Do a GET request to obtain the image\n",
    "        try:\n",
    "            response = requests.get(link, stream = True)\n",
    "        except Exception as e:\n",
    "                print('Image could not be found: '+link)\n",
    "                handle.close()\n",
    "                os.unlink(path)\n",
    "                continue\n",
    "    # Write the block (binary code) to the image file    \n",
    "        for block in response.iter_content(1024):\n",
    "            if not block:\n",
    "                break\n",
    "            handle.write(block)\n",
    "        handle.close()\n",
    "        n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Image could not be found: https://static/images/wikimedia-button.png\n",
      "Image could not be found: https://static/images/poweredby_mediawiki_88x31.png\n"
     ]
    }
   ],
   "source": [
    "ImageScraper('https://en.wikipedia.org/wiki/Main_Page',r'C:\\Users\\MeganDuffy\\OneDrive - Kubrick Group\\Week 09 - Python APIs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
